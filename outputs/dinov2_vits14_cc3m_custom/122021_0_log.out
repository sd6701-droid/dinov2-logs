submitit INFO (2025-11-28 18:16:22,128) - Starting with JobEnvironment(job_id=122021, hostname=b-31-113, local_rank=0(1), node=0(1), global_rank=0(1))
submitit INFO (2025-11-28 18:16:22,129) - Loading pickle: /home/sd6701/fall2025_deeplearning/outputs/dinov2_vits14_cc3m_custom/122021_submitted.pkl
I20251128 18:16:44 5393 dinov2 config.py:59] git:
  sha: 209ab45026fada7903dc614dfcc21ee889037174, status: clean, branch: main

I20251128 18:16:44 5393 dinov2 config.py:60] comment: 
config_file: dinov2/configs/train/custom.yaml
eval: 
eval_only: False
exclude: 
ngpus: 1
no_resume: False
nodes: 1
opts: ['train.dataset_path=cc3m:split=TRAIN:root=/home/sd6701/datasets/fall2025_deeplearning/cc3m_96px/train', 'optim.epochs=100', 'optim.warmup_epochs=10', 'train.batch_size_per_gpu=16', 'train.output_dir=/home/sd6701/fall2025_deeplearning/outputs/dinov2_vits14_cc3m_custom']
output_dir: /home/sd6701/fall2025_deeplearning/outputs/dinov2_vits14_cc3m_custom
partition: g2-standard-12
timeout: 600
use_volta32: False
I20251128 18:16:44 5393 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0002, new: 2.5e-05
I20251128 18:16:44 5393 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 4096
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 4096
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 16
  dataset_path: cc3m:split=TRAIN:root=/home/sd6701/datasets/fall2025_deeplearning/cc3m_96px/train
  output_dir: /home/sd6701/fall2025_deeplearning/outputs/dinov2_vits14_cc3m_custom
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_small
  patch_size: 14
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: swiglufused
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 100
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0002
  lr: 2.5e-05
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 98
evaluation:
  eval_period_iterations: 12500

I20251128 18:16:44 5393 dinov2 vision_transformer.py:126] using SwiGLU layer as FFN
I20251128 18:16:44 5393 dinov2 vision_transformer.py:126] using SwiGLU layer as FFN
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 384
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 4096
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:96] OPTIONS -- IBOT -- loss_weight: 1.0
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- head_n_prototypes: 4096
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_small network.
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:391] DISTRIBUTED FSDP -- preparing model for distributed training
W20251128 18:16:45 5393 py.warnings warnings.py:109] /home/sd6701/miniconda3/envs/dinov2/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py:295: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251128 18:16:45 5393 dinov2 train.py:303] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=384, out_features=2048, bias=True)
                  (w3): Linear(in_features=1024, out_features=384, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=384, out_features=2048, bias=True)
                  (w3): Linear(in_features=1024, out_features=384, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=384, out_features=2048, bias=True)
                  (w3): Linear(in_features=1024, out_features=384, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=384, out_features=2048, bias=True)
                  (w3): Linear(in_features=1024, out_features=384, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=384, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=4096, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=384, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=4096, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=384, out_features=2048, bias=True)
                  (w3): Linear(in_features=1024, out_features=384, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=384, out_features=2048, bias=True)
                  (w3): Linear(in_features=1024, out_features=384, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=384, out_features=2048, bias=True)
                  (w3): Linear(in_features=1024, out_features=384, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=384, out_features=1152, bias=True)
                  (proj): Linear(in_features=384, out_features=384, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                (mlp): SwiGLUFFNFused(
                  (w12): Linear(in_features=384, out_features=2048, bias=True)
                  (w3): Linear(in_features=1024, out_features=384, bias=True)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=384, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=4096, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=384, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=4096, bias=False)
      )
    )
  )
)
I20251128 18:16:45 5393 dinov2 param_groups.py:54] chunked fsdp
I20251128 18:16:45 5393 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.1.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.2.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.mlp.w12.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.mlp.w12.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.mlp.w3.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.mlp.w3.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] blocks.3.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:378] fusing param groups
I20251128 18:16:45 5393 dinov2 param_groups.py:64] else code branch
I20251128 18:16:45 5393 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:378] fusing param groups
I20251128 18:16:45 5393 dinov2 param_groups.py:64] else code branch
I20251128 18:16:45 5393 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251128 18:16:45 5393 dinov2 ssl_meta_arch.py:378] fusing param groups
I20251128 18:16:45 5393 dinov2 train.py:102] Schedulers ready.
I20251128 18:16:45 5393 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251128 18:16:45 5393 dinov2 augmentations.py:34] ###################################
I20251128 18:16:45 5393 dinov2 augmentations.py:35] Using data augmentation parameters:
I20251128 18:16:45 5393 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20251128 18:16:45 5393 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20251128 18:16:45 5393 dinov2 augmentations.py:38] local_crops_number: 8
I20251128 18:16:45 5393 dinov2 augmentations.py:39] global_crops_size: 224
I20251128 18:16:45 5393 dinov2 augmentations.py:40] local_crops_size: 98
I20251128 18:16:45 5393 dinov2 augmentations.py:41] ###################################
I20251128 18:16:45 5393 dinov2 loaders.py:88] using dataset: "cc3m:split=TRAIN:root=/home/sd6701/datasets/fall2025_deeplearning/cc3m_96px/train"
I20251128 18:24:30 5393 dinov2 loaders.py:93] # of dataset samples: 430,338
I20251128 18:24:30 5393 dinov2 loaders.py:126] sampler: sharded infinite
I20251128 18:24:30 5393 dinov2 loaders.py:210] using PyTorch data loader
I20251128 18:24:30 5393 dinov2 loaders.py:225] infinite data loader
I20251128 18:24:30 5393 dinov2 train.py:217] Starting training from iteration 0
W20251128 18:24:45 5393 py.warnings warnings.py:109] /home/sd6701/miniconda3/envs/dinov2/lib/python3.10/site-packages/xformers/ops/unbind.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage_data_ptr = tensors[0].storage().data_ptr()

W20251128 18:24:45 5393 py.warnings warnings.py:109] /home/sd6701/miniconda3/envs/dinov2/lib/python3.10/site-packages/xformers/ops/unbind.py:48: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  if x.storage().data_ptr() != storage_data_ptr:

submitit ERROR (2025-11-28 18:24:45,785) - Submitted job triggered an exception
E20251128 18:24:45 5393 submitit submission.py:68] Submitted job triggered an exception
